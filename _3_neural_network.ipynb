{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xixihaha1995/esp_proj3/blob/main/_3_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWo1A1O650Wn"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_background(color):    \n",
        "    script = (\n",
        "        \"var cell = this.closest('.jp-CodeCell');\"\n",
        "        \"var editor = cell.querySelector('.jp-Editor');\"\n",
        "        \"editor.style.background='{}';\"\n",
        "        \"this.parentNode.removeChild(this)\"\n",
        "    ).format(color)\n",
        "    \n",
        "    display(HTML('<img src onerror=\"{}\" style=\"display:none\">'.format(script)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ],
      "metadata": {
        "id": "Kn61XnUHAayz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus, a neural network is either a **biological** neural network, made up of biological neurons, or an **artificial** neural network, used for solving artificial intelligence (AI) problems.<br>\n",
        "\n",
        "So, each node represnet one neuron, the formula for value coming out from the neuron 2 in the hidden layer is below:<br>\n",
        "$a_{hidden,2} = activation_{function}(w_{1}a_{input,1} + w_{2}a_{input, 2} + bias)$<br>\n",
        "where $a$ stands for the **activation**(amplitude/magnitude/value)  coming out from the neuron, which is controlled by the $activation_{function}$<br>\n",
        "Some [common activation](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) functions:<br>\n",
        "\n",
        "\n",
        "1.   Identity. $Identity(x) = x$\n",
        "2.   Binary step. $Binary_{step}(x) = \\begin{cases} \n",
        "      0 & x\\leq 0 \\\\\n",
        "      1 & x > 0\n",
        "   \\end{cases}$\n",
        "3. Logistic(or sigmoid, $σ$, soft step). $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
        "4. Others (**ReLU**, **tanh**):\n",
        "![Common activation functions](https://drive.google.com/uc?export=view&id=1xe8pJv_gAm0kIBgccWVZa0UCD1t56_Kh)"
      ],
      "metadata": {
        "id": "iZUvfpX0A-RT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks\n",
        "References: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by colah'blog.<br>\n",
        "1. Traditional neural network cannot use previous events to inform later ones. TO be specific, traditional neural network will integrate all the information from initial time till the end time.\n",
        "![Traditional neural network](https://drive.google.com/uc?export=view&id=1gQoLLUb8u4ZSWAzTenykECongVTcxNi-)\n",
        "2. Recurrent neural networks address this issue. RNN allow information from the past to persist.\n",
        "![An unrolled recurrent neural network.](https://drive.google.com/uc?export=view&id=1CWZjuBwIiA50s6dEUki_CJNgeX7h27Ue)\n",
        "3. The problem of long-term dependecies. Sometimes, we only need to look at recent information to perform the present task. For example, if we are trying to predict the last word in \"the clouds are in the ***__***\", it's pretty obvious the word is going to be ***sky***.<br>\n",
        "Unfortunately, as the gap between the relevant information and the place that it's needed grows, RNNs become unable to learn to connect the information. For example, trying to predict the last word in the text “I grew up in France… I speak fluent ***French***.”\n",
        "4. RNN, LSTM, GRU\n",
        "Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by [Hochreiter & Schmidhuber (1997) ](http://www.bioinf.jku.at/publications/older/2604.pdf)<br>\n",
        "A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by [Cho, et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf).\n",
        "![Simple RNN structure](https://drive.google.com/uc?export=view&id=1a3mEns6mvwkySJ0bdcYgJimwKBHzBgfg)\n",
        "![LSTM structure](https://drive.google.com/uc?export=view&id=13fNKVSLdTJctnw5vM8wYQljJPA-NlJEq)\n",
        "![GRU structure](https://drive.google.com/uc?export=view&id=1_Uc6o7v1u_7ea1szvAV5ABjs74-mkTbt)"
      ],
      "metadata": {
        "id": "AaO_NbaVJB_x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkvqF9jG50Wr"
      },
      "source": [
        "\n",
        "Defining a Neural Network in PyTorch\n",
        "====================================\n",
        "Deep learning uses artificial neural networks (models), which are\n",
        "computing systems that are composed of many layers of interconnected\n",
        "units. By passing data through these interconnected units, a neural\n",
        "network is able to learn how to approximate the computations required to\n",
        "transform inputs into outputs. In PyTorch, neural networks can be\n",
        "constructed using the ``torch.nn`` package.\n",
        "\n",
        "Introduction\n",
        "------------\n",
        "PyTorch provides the elegantly designed modules and classes, including\n",
        "``torch.nn``, to help you create and train neural networks. An\n",
        "``nn.Module`` contains layers, and a method ``forward(input)`` that\n",
        "returns the ``output``.\n",
        "\n",
        "In this recipe, we will use ``torch.nn`` to define a neural network\n",
        "intended for the `MNIST\n",
        "dataset <https://pytorch.org/docs/stable/torchvision/datasets.html#mnist>`__.\n",
        "\n",
        "Setup\n",
        "-----\n",
        "Before we begin, we need to install ``torch`` if it isn’t already\n",
        "available.\n",
        "\n",
        "::\n",
        "\n",
        "   pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lXh2kJY50Wt"
      },
      "source": [
        "Steps\n",
        "-----\n",
        "\n",
        "1. Import all necessary libraries for loading our data\n",
        "2. Define and initialize the neural network\n",
        "3. Specify how data will pass through your model\n",
        "4. [Optional] Pass data through your model to test\n",
        "\n",
        "1. Import necessary libraries for loading our data\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "For this recipe, we will use ``torch`` and its subsidiaries ``torch.nn``\n",
        "and ``torch.nn.functional``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68zUNX6I50Wu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "'''\n",
        "While the former defines nn.Module classes, the latter uses a functional (stateless) approach.\n",
        "To dig a bit deeper: nn.Modules are defined as Python classes and have attributes, \n",
        "e.g. a nn.Conv2d module will have some internal attributes like self.weight. \n",
        "F.conv2d however just defines the operation and needs all arguments to be passed (including the weights and bias).\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laG7-Y6550Wv"
      },
      "source": [
        "2. Define and intialize the neural network\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Our network will recognize images. We will use a process built into\n",
        "PyTorch called convolution. Convolution adds each element of an image to\n",
        "its local neighbors, weighted by a kernel, or a small matrix, that\n",
        "helps us extract certain features (like edge detection, sharpness,\n",
        "blurriness, etc.) from the input image.\n",
        "\n",
        "There are two requirements for defining the ``Net`` class of your model.\n",
        "The first is writing an **``__init__``** function that references\n",
        "``nn.Module``. This function is where you define the fully connected\n",
        "layers in your neural network.\n",
        "\n",
        "Using convolution, we will define our model to take 1 input image\n",
        "channel, and output match our target of 10 labels representing numbers 0\n",
        "through 9. This algorithm is yours to create, we will follow a standard\n",
        "MNIST algorithm.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCBTycYh50Ww"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # First 2D convolutional layer, taking in 1 input channel (image),\n",
        "      # outputting 32 convolutional features, with a square kernel size of 3\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "      # Second 2D convolutional layer, taking in the 32 input layers,\n",
        "      # outputting 64 convolutional features, with a square kernel size of 3\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "\n",
        "      # Designed to ensure that adjacent pixels are either all 0s or all active\n",
        "      # with an input probability\n",
        "      self.dropout1 = nn.Dropout2d(0.25)\n",
        "      self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "      # First fully connected layer\n",
        "      self.fc1 = nn.Linear(9216, 128)\n",
        "      # Second fully connected layer that outputs our 10 labels\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "my_nn = Net()\n",
        "print(my_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neDDqb4K50Wy"
      },
      "source": [
        "We have finished defining our neural network, now we have to define how\n",
        "our data will pass through it.\n",
        "\n",
        "3. Specify how data will pass through your model\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "When you use PyTorch to build a model, you just **have to **define the\n",
        "``forward`` function, that will pass the data into the computation graph\n",
        "(i.e. our neural network). This will represent our feed-forward\n",
        "algorithm.\n",
        "\n",
        "You can use any of the Tensor operations in the ``forward`` function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPVirXTO50Wz"
      },
      "outputs": [],
      "source": [
        "class OneHiddenLayerNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=drop_prob_):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "  \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.hidden(x)\n",
        "        x = self.output(self.relu(x))\n",
        "        return x\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=drop_prob_):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n",
        "\n",
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=drop_prob_):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "      self.dropout1 = nn.Dropout2d(0.25)\n",
        "      self.dropout2 = nn.Dropout2d(0.5)\n",
        "      self.fc1 = nn.Linear(9216, 128)\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "      # Pass data through conv1\n",
        "      x = self.conv1(x)\n",
        "      # Use the rectified-linear activation function over x\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      # Run max pooling over x\n",
        "      x = F.max_pool2d(x, 2)\n",
        "      # Pass data through dropout1\n",
        "      x = self.dropout1(x)\n",
        "      # Flatten x with start_dim=1\n",
        "      x = torch.flatten(x, 1)\n",
        "      # Pass data through fc1\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.dropout2(x)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      # Apply softmax to x \n",
        "      output = F.log_softmax(x, dim=1)\n",
        "      return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lbsObZq50W1"
      },
      "source": [
        "4. [Optional] Pass data through your model to test\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "To ensure we receive our desired output, let’s test our model by passing\n",
        "some random data through it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qUbPPgj50W2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70989269-e4a1-48c7-8729-ff8a56997617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.2414, -2.2682, -2.3167, -2.2478, -2.4035, -2.2451, -2.2415, -2.3222,\n",
            "         -2.3839, -2.3740]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# Equates to one random 28x28 image\n",
        "random_data = torch.rand((1, 1, 28, 28))\n",
        "\n",
        "my_nn = Net()\n",
        "result = my_nn(random_data)\n",
        "print (result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiRMCA8v50W4"
      },
      "source": [
        "Each number in this resulting tensor equates to the prediction of the\n",
        "label the random tensor is associated to.\n",
        "\n",
        "Congratulations! You have successfully defined a neural network in\n",
        "PyTorch.\n",
        "\n",
        "Learn More\n",
        "----------\n",
        "\n",
        "Take a look at these other recipes to continue your learning:\n",
        "\n",
        "- `What is a state_dict in PyTorch <https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html>`__\n",
        "- `Saving and loading models for inference in PyTorch <https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html>`__\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "_3_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}